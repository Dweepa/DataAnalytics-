{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#from fancyimpute import KNN\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#sklearn models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#one hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for knn imputation\n",
    "### Did not work: memory error issue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        #print \"The continuous distance \" + numeric_distance + \" is not supported.\"\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        #print \"The binary distance \" + categorical_distance + \" is not supported.\"\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        #print \"Not enough observations.\"\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        #print \"The number of observations in the attributes variable is not matching the target variable length.\"\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        #print \"The range of the number of neighbors is incorrect.\"\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        #print \"The aggregation method is incorrect.\"\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        #print \"The only method allowed for categorical target variable is the mode.\"\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Dataset/fully_encoded.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Plot\n",
    "### Removed columns with correlation against case status less than 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10a0e7668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = data.corr()\n",
    "plt.matshow(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 57, True: 54})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_case_status = correlation[\"case_status\"]\n",
    "low_val = corr_case_status>0.0001\n",
    "Counter(list(low_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "new_data = pd.DataFrame()\n",
    "for a in columns:\n",
    "    if a=='class_of_admission' or correlation['case_status'][a]>0.0001:\n",
    "        new_data[a] = data[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data.drop(['class_of_admission', 'case_status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN attempted for imputation. Did not work due to memory issue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knn_impute(target=new_data['case_status'], attributes=new_data.drop(['case_status'], 1),\n",
    "                                    aggregation_method=\"median\", k_neighbors=10, numeric_distance='euclidean',\n",
    "                                    categorical_distance='hamming', missing_neighbors_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = new_data.fillna(value = new_data.mean().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance:  [0.03683722 0.03155411 0.02712954 0.02285996 0.02261256 0.02245898\n",
      " 0.0219014  0.02142511 0.02121099 0.02070546 0.02031272 0.02014971\n",
      " 0.02000689 0.0199427  0.01969017 0.01966167 0.01952625 0.01943864\n",
      " 0.01938724 0.01922036 0.01912021 0.01906894 0.01902004 0.0190001\n",
      " 0.01896258 0.01892937 0.01891178 0.01888543 0.01887545 0.0188138\n",
      " 0.0187728  0.01873756 0.01869577 0.01865467 0.01855741 0.0184743\n",
      " 0.01843658 0.01822532 0.01806544 0.01788895]\n",
      "total:  82.0128170555737\n"
     ]
    }
   ],
   "source": [
    "X_normalized = StandardScaler().fit_transform(new_data)\n",
    "pca = PCA(n_components = 40)\n",
    "principalComponents = pca.fit_transform(X_normalized)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "print(\"explained variance: \", pca.explained_variance_ratio_)\n",
    "print(\"total: \", sum(pca.explained_variance_ratio_)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of PCA\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Number of Components</td><td>Explained Variance %</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>25</td><td>53.5489</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>30</td><td>63.1431</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>35</td><td>72.6216</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>40</td><td>82.004</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>45</td><td>90.574</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>50</td><td>97.7323</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf['case_status'] = data['case_status']\n",
    "def fun(x):\n",
    "    d = {'H-1B':0,'L-1':1,'F-1':2}\n",
    "    return d[x]\n",
    "principalDf['class_of_admission'] = data['class_of_admission'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf.to_csv(\"../Dataset/afterPCA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
